{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from config import MODELS_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "meta_path_path = \"meta_data.txt\"\n",
    "metadata = {}\n",
    "with open(meta_path_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        info_dict = dict()\n",
    "        str_line = line\n",
    "        data = str_line.split(';')\n",
    "        filename = data[0].split('\\t')[0]\n",
    "        item = data[0].split('\\t')[1].split('=')\n",
    "        info_dict[item[0]] = item[1]\n",
    "        for entry in data[1:]:\n",
    "            items = entry.split('=')\n",
    "            info_dict[items[0].strip()] = items[1].strip()\n",
    "        metadata[filename] = info_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell classification, total files 520, 9 classes\n",
      "antibody classification, total files 212, 7 classes\n"
     ]
    }
   ],
   "source": [
    "# select files for classification\n",
    "def sel_files(metadata, label_type):\n",
    "    file_list = list(metadata.keys())\n",
    "    labels = []\n",
    "    for f in file_list:\n",
    "        if metadata[f][label_type] == '':\n",
    "            raise ValueError(f\"No {label_type} type\")\n",
    "        labels.append(metadata[f][label_type])\n",
    "    counter = Counter(labels)\n",
    "    sel_classes = []\n",
    "    for cls, freq in counter.most_common():\n",
    "        if freq > 10:\n",
    "            sel_classes.append(cls)\n",
    "\n",
    "    sel_file_labels = {c:[] for c in sel_classes}\n",
    "    total_files = 0\n",
    "    for f in file_list:\n",
    "        label = metadata[f][label_type]\n",
    "        if label in sel_file_labels:\n",
    "            sel_file_labels[label].append(f)\n",
    "            total_files += 1\n",
    "    print(f\"{label_type} classification, total files {total_files}, {len(sel_classes)} classes\")\n",
    "    return sel_file_labels\n",
    "cell_data = sel_files(metadata, \"cell\")\n",
    "antibody_data = sel_files(metadata, \"antibody\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "def split_data(data_array, train_ratio=0.6):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for key in data_array:\n",
    "        num = len(data_array[key])\n",
    "        train_num = int(num*train_ratio)\n",
    "        assert train_num > 0 and train_num < num, \"invalid training number\"\n",
    "        indexes = np.arange(num)\n",
    "        np.random.shuffle(indexes)\n",
    "        train_data.extend([(data_array[key][i],key) for i in indexes[0:train_num]])\n",
    "        test_data.extend([(data_array[key][i],key) for i in indexes[train_num:]])\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training and test files\n",
    "# os.makedirs(\"classification_data\", exist_ok=True)\n",
    "# for it in range(5):\n",
    "#     train_ratio = 0.6\n",
    "#     train_cell_data, test_cell_data = split_data(cell_data, train_ratio)\n",
    "#     with open(os.path.join(\"classification_data\", f\"cell_train_train{train_ratio:.2f}_{it}.txt\"), \"w\") as f:\n",
    "#         for fname, l in train_cell_data:\n",
    "#             f.write(f\"{fname}\\t{l}\\n\")\n",
    "#     with open(os.path.join(\"classification_data\", f\"cell_test_train{train_ratio:.2f}_{it}.txt\"), \"w\") as f:\n",
    "#         for fname, l in test_cell_data:\n",
    "#             f.write(f\"{fname}\\t{l}\\n\")\n",
    "\n",
    "# for it in range(5):\n",
    "#     train_ratio = 0.6\n",
    "#     train_antibody_data, test_antibody_data = split_data(antibody_data, train_ratio)\n",
    "#     with open(os.path.join(\"classification_data\", f\"antibody_train_train{train_ratio:.2f}_{it}.txt\"), \"w\") as f:\n",
    "#         for fname, l in train_antibody_data:\n",
    "#             f.write(f\"{fname}\\t{l}\\n\")\n",
    "#     with open(os.path.join(\"classification_data\", f\"antibody_test_train{train_ratio:.2f}_{it}.txt\"), \"w\") as f:\n",
    "#         for fname, l in test_antibody_data:\n",
    "#             f.write(f\"{fname}\\t{l}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('K562', 90), ('GM12878', 54), ('HepG2', 46), ('HeLa-S3', 38), ('H1-hESC', 34), ('A549', 21), ('MCF-7', 10), ('HUVEC', 8), ('MCF10A-Er-Src', 8)]\n",
      "[('K562', 60), ('GM12878', 36), ('HepG2', 31), ('HeLa-S3', 26), ('H1-hESC', 24), ('A549', 14), ('MCF-7', 8), ('HUVEC', 6), ('MCF10A-Er-Src', 6)]\n",
      "520\n"
     ]
    }
   ],
   "source": [
    "# load training and test data from files\n",
    "def load_data(data_path):\n",
    "    fnames = []\n",
    "    labels = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            fname, label = line.strip().split('\\t')\n",
    "            fnames.append(fname)\n",
    "            labels.append(label)\n",
    "    return fnames, labels\n",
    "train_files = \"classification_data/cell_train_train0.60_0.txt\"\n",
    "test_files = \"classification_data/cell_test_train0.60_0.txt\"\n",
    "\n",
    "train_data, train_label = load_data(train_files)\n",
    "test_data, test_label = load_data(test_files)\n",
    "print(Counter(train_label).most_common())\n",
    "print(Counter(test_label).most_common())\n",
    "print(len(set(train_data).union(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "def get_file_embedding(file_path, model):\n",
    "    sentence = []\n",
    "    embedding = 0.0\n",
    "    count = 0\n",
    "    vocab2index = model.wv.key_to_index\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            elements = line.strip().split('\\t')[0:3]\n",
    "            chr_name = elements[0].strip()\n",
    "            start = elements[1].strip()\n",
    "            end = elements[2].strip()\n",
    "            word = chr_name+':'+start+'-'+end\n",
    "            if word in vocab2index:\n",
    "                embedding += model.wv.vectors[vocab2index[word]]\n",
    "                count += 1\n",
    "    if count > 0:\n",
    "        embedding /= count\n",
    "    return embedding\n",
    "def get_embeddings(data, model_path, universe):\n",
    "    model = Word2Vec.load(model_path)\n",
    "    token_folder = os.path.join(f\"{MODELS_FOLDER}/tokens\", f\"token_universe_{universe}\")\n",
    "    embeddings = []\n",
    "    for fname in data:\n",
    "        fname = fname[0:-3]\n",
    "        file_path = os.path.join(token_folder, fname)\n",
    "        embed = get_file_embedding(file_path, model)\n",
    "        embeddings.append(embed)\n",
    "    return embeddings\n",
    "def get_label2index(labels):\n",
    "    label2index = {}\n",
    "    count = 0\n",
    "    for l in labels:\n",
    "        if l not in label2index:\n",
    "            label2index[l] = count\n",
    "            count += 1\n",
    "    return label2index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniitialize the prediction model\n",
    "def get_classification_score(X_train, y_train, X_test, y_test):\n",
    "    clf = svm.SVC(kernel = 'linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    f1 = f1_score((y_test), clf.predict(X_test), average = 'micro')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{MODELS_FOLDER}/expr_universe_Large/50W10D-0.1000r/models/region2vec_latest.pt\"\n",
    "\n",
    "file2idx = {f:i for i, f in enumerate(train_data+test_data)}\n",
    "label2number = get_label2index(train_label)\n",
    "\n",
    "train_embeddings = get_embeddings(train_data, model_path, \"Large\")\n",
    "test_embeddings = get_embeddings(test_data, model_path, \"Large\")\n",
    "embeddings = np.concatenate([train_embeddings, test_embeddings])\n",
    "\n",
    "train_label_number = [label2number[l] for l in train_label]\n",
    "test_label_number = [label2number[l] for l in test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_classificatoin_score(model_path, num_runs=5):\n",
    "    all_scores = []\n",
    "    for cls_type in [\"cell\", \"antibody\"]:\n",
    "        cls_scores = []\n",
    "        for n in range(num_runs):\n",
    "            train_files = f\"classification_data/{cls_type}_train_train0.60_{n}.txt\"\n",
    "            test_files = f\"classification_data/{cls_type}_test_train0.60_{n}.txt\"\n",
    "            train_data, train_label = load_data(train_files)\n",
    "            test_data, test_label = load_data(test_files)\n",
    "            if n == 0: # get all embeddings and labels once\n",
    "                file2idx = {f:i for i, f in enumerate(train_data+test_data)}\n",
    "                label2number = get_label2index(train_label)\n",
    "                train_embeddings = get_embeddings(train_data, model_path, \"Large\")\n",
    "                test_embeddings = get_embeddings(test_data, model_path, \"Large\")\n",
    "                embeddings = np.concatenate([train_embeddings, test_embeddings])\n",
    "            train_embeddings = np.stack([embeddings[file2idx[f]] for f in train_data])\n",
    "            test_embeddings = np.stack([embeddings[file2idx[f]] for f in test_data])\n",
    "            train_label_number = [label2number[l] for l in train_label]\n",
    "            test_label_number = [label2number[l] for l in test_label]\n",
    "            f1 = get_classification_score(train_embeddings, train_label_number, test_embeddings, test_label_number)\n",
    "            cls_scores.append(f1)\n",
    "        cls_scores = np.array(cls_scores)\n",
    "        avg_score = cls_scores.mean()\n",
    "        std_score = cls_scores.std()\n",
    "        all_scores.append((avg_score,std_score))\n",
    "    return all_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9080568720379147, 0.017884324420960447) (0.885057471264368, 0.021808811449437106)\n"
     ]
    }
   ],
   "source": [
    "cell_score, antibody_score = get_avg_classificatoin_score(model_path)\n",
    "print(cell_score, antibody_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
